{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04-01_Vectorization.ipynb의 사본","provenance":[{"file_id":"1FeuBTghMp-W-9mxk6f9hYSqiLtTLsNo8","timestamp":1658911692078}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 머신 모델 입력은 숫자 형태만 가능하다\n","\n","**Integer Encoding** 과정 필요\n","\n","[가, 나, 다] ==> [0, 1, 2]\n","\n","[남, 여] ==> [0, 1]\n","\n","이산데이터이므로 인코딩된 숫자도 **숫자 끼리의 산술적 관계는 없음**\n","\n","(2는 0보다 큰 값이다, 0은 1보다 작은 값이다, 같은 해석을 하지 않는다는 뜻)\n","\n","0로 분류, 1로 분류, 2로 분류 \"분류\" 로서의 기능만을 수행"],"metadata":{"id":"smoSNjJ04fx8"}},{"cell_type":"markdown","source":["# X가 단어 데이터인 경우\n","\n","Integer Encoding + 벡터화를 해서\n","\n","숫자 끼리의 산술적 관계를 의미있게 할 수 있다"],"metadata":{"id":"k6SHa-GF5f5Q"}},{"cell_type":"markdown","source":["## Integer Encoding\n","\n","<pre>\n","[바나나, 책,    자동차,   사과,    노트,  트럭] \n","\n","==>\n","\n","[0,      1,      2,       3,       4,      5]\n","</pre>"],"metadata":{"id":"z7ugR43z53pB"}},{"cell_type":"code","source":["vocabulary = {'바나나': 0,\n","              '책'    : 1,\n","              '자동차': 2,\n","              '사과'  : 3,\n","              '노트'  : 4,\n","              '트럭'  : 5}"],"metadata":{"id":"9HWxSlkg7wdV","executionInfo":{"status":"ok","timestamp":1658975837357,"user_tz":-540,"elapsed":279,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["vocabulary['바나나']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HB_C8bFG8Bci","executionInfo":{"status":"ok","timestamp":1658975839301,"user_tz":-540,"elapsed":291,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}},"outputId":"22a71559-f2f6-41fa-beaf-3b95ff75cf1b"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["## Vectorization\n","\n","5차원의 특성을 실수로 표현하는 벡터로 정의한다면,\n","\n","[색깔, 무게, 당도, 속도, 재질]\n","\n","\n","바나나 분석  -->  벡터화함수(0)  --> [2.0, 0.5, 2.3, 0.0, 0.3]\n","\n","책 분석  -->  벡터화함수(1)  --> [2.8, 1.2, 0.0, 0.0, 1.5]\n","\n","자동차 분석  -->  벡터화함수(2)  --> [8, 700.3, 0.0, 150.0, 5.5]\n","\n","사과 분석  -->  벡터화함수(3)  --> [1.0, 0.35, 2.2, 0.0, 0.4]\n","\n","노트 분석  -->  벡터화함수(4)  --> [2.9, 1.1, 0.0, 0.0, 1.48]\n","\n","트럭 분석  -->  벡터화함수(5)  --> [9, 800.0, 0.0, 130.0, 6.0]\n","\n","\n","\n"],"metadata":{"id":"dHuTITH36FJf"}},{"cell_type":"markdown","source":["벡터끼리는 유사도 측정 가능\n","\n","코사인 유사도  \n","유클리드 거리  \n","자카드 유사도  \n","..\n"],"metadata":{"id":"XK4RTCUr8hQP"}},{"cell_type":"code","source":["import numpy as np\n","\n","def euclidean(x, y):\n","    return np.sqrt(np.sum(np.square(x - y)))"],"metadata":{"id":"DZzEHBym8Rxu","executionInfo":{"status":"ok","timestamp":1658975841551,"user_tz":-540,"elapsed":427,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["a = np.array((1, 2))\n","b = np.array((1, 4))\n","c = np.array((4, 5))\n","\n","print(euclidean(a, b))\n","print(euclidean(a, c))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHCXxOV29DUh","executionInfo":{"status":"ok","timestamp":1658975842848,"user_tz":-540,"elapsed":297,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}},"outputId":"c62e8f58-8b66-41b5-c5ab-bd61c1c28c68"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0\n","4.242640687119285\n"]}]},{"cell_type":"code","source":["from scipy.spatial import distance\n","\n","print(distance.euclidean(a, b))\n","print(distance.euclidean(a, c))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKzkQHOv9I84","executionInfo":{"status":"ok","timestamp":1658975845795,"user_tz":-540,"elapsed":446,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}},"outputId":"4f03f0f0-810a-4824-f6cb-623de48442e7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0\n","4.242640687119285\n"]}]},{"cell_type":"code","source":["word_vector = [\n","    [2.0, 0.5, 2.3, 0.0, 0.3],    # 바나나\n","    [2.8, 1.2, 0.0, 0.0, 1.5],    # 책\n","    [8, 700.3, 0.0, 150.0, 5.5],  # 자동차\n","    [1.0, 0.35, 2.2, 0.0, 0.4],   # 사과\n","    [2.9, 1.1, 0.0, 0.0, 1.48],   # 노트\n","    [9, 800.0, 0.0, 130.0, 6.0]   # 트럭\n","]\n","\n","vocab_size = 6\n","\n","vocabulary = {'바나나': 0,\n","              '책'    : 1,\n","              '자동차': 2,\n","              '사과'  : 3,\n","              '노트'  : 4,\n","              '트럭'  : 5}\n","\n","index_to_word = {0: '바나나',\n","                 1: '책',\n","                 2: '자동차',\n","                 3: '사과',\n","                 4: '노트',\n","                 5: '트럭'}\n","\n","def get_similar_word(word):\n","\n","    similarity = []\n","\n","    input_idx = vocabulary[word]         # 1\n","    input_vec = word_vector[input_idx]   # [2.8, 1.2, 0.0, 0.0, 1.5]\n","\n","    for i in range(vocab_size):\n","        if i == input_idx:\n","            val = float('inf')\n","        else:\n","            val = distance.euclidean(input_vec, word_vector[i])\n","        similarity.append(val)\n","\n","    print(similarity) # [ 0.5  inf  1.5 2.7  0.3  5.5 ]\n","    result_idx = np.argmin(similarity, axis=-1)   # 유클리드 값이 가장 작은 값 리턴 2\n","    print(result_idx)\n","    print(index_to_word[result_idx])\n","\n","\n"],"metadata":{"id":"flg64gG99tEU","executionInfo":{"status":"ok","timestamp":1658975953896,"user_tz":-540,"elapsed":292,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["get_similar_word('트럭')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHQWRrsH-xmh","executionInfo":{"status":"ok","timestamp":1658975958468,"user_tz":-540,"elapsed":281,"user":{"displayName":"dayeon Kim","userId":"10050630958817967118"}},"outputId":"b59ebaf4-101d-4e7c-8a42-373bb1237220"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[810.0537204408113, 809.3454948290007, 101.69237926216502, 810.2100483825167, 809.4435436767657, inf]\n","2\n","자동차\n"]}]},{"cell_type":"markdown","source":["# 딥러닝 이전 벡터화 방법\n","\n","TF-IDF  \n","\n","Document-Term Matrix\n","\n","Topic Modeling (LSI, LSA, LDA)"],"metadata":{"id":"QmVtmh56Aaf6"}},{"cell_type":"markdown","source":["# 딥러닝 기법\n","\n","Word Embedding Layer 를 모델 레이어로 추가하면 됨\n","\n"],"metadata":{"id":"9c0psdiGAnZG"}},{"cell_type":"markdown","source":["\n","**모델 입력**\n","\n","1) Integer Encoding 까지만 처리 (필요한 경우 zero padding, 이건 나중에)\n","\n","단어의 코드에 불과한 인코딩숫자 (0, 1, 2, ... vocab 개수 만큼) 만 생성\n","\n","<pre>\n","[바나나, 책,    자동차,   사과,    노트,  트럭, ...] \n","\n","==>\n","\n","[0,      1,      2,       3,       4,      5, ...]\n","</pre>\n","\n","\n","2) 딥러닝 모델에 Embedding Layer 추가하고 (Dense() 추가 하듯)\n","\n","만들 벡터의 크기 (예 5차원 벡터)만 정해주면 됨\n","\n"],"metadata":{"id":"V74JIKd5B2G-"}},{"cell_type":"markdown","source":["**모델 학습 결과**\n","\n","모든 vocabulary 단어에 대해 각각 5차원의 벡터를 만들어준다\n","\n","<pre>\n","EMBEDDING Lookup Table 만들어줌\n","\n","    [2.0, 0.5, 2.3, 0.0, 0.3],    # 바나나\n","    [2.8, 1.2, 0.0, 0.0, 1.5],    # 책\n","    [8, 700.3, 0.0, 150.0, 5.5],  # 자동차\n","    [1.0, 0.35, 2.2, 0.0, 0.4],   # 사과\n","    [2.9, 1.1, 0.0, 0.0, 1.48],   # 노트\n","    [9, 800.0, 0.0, 130.0, 6.0]   # 트럭\n","     ...\n","</pre>"],"metadata":{"id":"h5ABkBQLCOmj"}},{"cell_type":"code","source":[""],"metadata":{"id":"Adp0uWwB_DDZ"},"execution_count":null,"outputs":[]}]}